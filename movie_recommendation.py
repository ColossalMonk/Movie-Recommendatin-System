# -*- coding: utf-8 -*-
"""movie_recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13u4xGIfZdNsJbA1Kha-hvlefQEykcHPx
"""

#We have two strings
# String 1 => London Paris London
# String 2 => Paris Paris London

#   / \
# P 5|
#    |                                                      distance => [(1,2),(2,1)]
# A 4|                                                      vector A => from [(0,0) to (1,2)]
#    |                                                      vector B => from [(0,0) to (2,1)]
# R 3|                                                                    ([vectorA].[vectorB])
#    |                                                      cos(theta) = ------------------------
# I 2|     x                                                              (|vectorA|.|vectorB|)
#    |       
# S 1|           x
#    |     
# ___|______________________________________________\
# 0  |     1     2     3     4     5  L O N D O N   /

#creating a list of two sentences
text = ["London Paris London", "Paris Paris London"]

#preffered output
[[2,1],[1,2]]

from sklearn.feature_extraction.text import CountVectorizer

text = ["London Paris London", "Paris Paris London"]

cv = CountVectorizer()

count_matrix = cv.fit_transform(text)

print(count_matrix)

#in the above matrix, the representation is that of sparse matrix
#it is saying that the value of matrix at [0,0] & [1,1] is 2 and at [0,1] & [1,0] is 1.

from sklearn.feature_extraction.text import CountVectorizer

text = ["London Paris London", "Paris Paris London"]

cv = CountVectorizer()

count_matrix = cv.fit_transform(text)

print(count_matrix.toarray())

# finding cosine similarities 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import  cosine_similarity

text = ["London Paris London", "Paris Paris London"]

cv = CountVectorizer()

count_matrix = cv.fit_transform(text)

similarity_score = cosine_similarity(count_matrix)

print (similarity_score)

# what this matrix implies is the similarity between the sentences
# the first and second sentence are similar to themselves completely and hence we get a value of 1
# on the position of [0,0] and [1,1], on the other hand the first sentence is similar to second
# sentence by value of 0.8 and vice varsa.

# when to use angular distance and when to use normal/eucledian distance
# graph

# movie recommendation starts
# dataset created from IMDB dataset for 5000 movies
# data has been cleaned

#viewing the data file
import pandas as pd

data = pd.read_csv("movie_dataset.csv")
data.head()

# bare bone structure of the code
# also this is build around content based movie recommendation

#import pandas as pd
#import numpy as np
#from sklearn.feature_extraction.text import CountVectorizer
#from sklearn.metrics.pairwise import cosine_similarity
###### helper functions. Use them when needed #######
#def get_title_from_index(index):
#	return df[df.index == index]["title"].values[0]

#def get_index_from_title(title):
#	return df[df.title == title]["index"].values[0]
##################################################

##Step 1: Read CSV File

##Step 2: Select Features

##Step 3: Create a column in DF which combines all selected features

##Step 4: Create count matrix from this new combined column

##Step 5: Compute the Cosine Similarity based on the count_matrix

#movie_user_likes = "Avatar"

## Step 6: Get index of this movie from its title

## Step 7: Get a list of similar movies in descending order of similarity score


## Step 8: Print titles of first 50 movies

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

###### helper functions. Use them when needed #######
def get_title_from_index(index):
	return df[df.index == index]["title"].values[0]

def get_index_from_title(title):
	return df[df.title == title]["index"].values[0]
##################################################

##Step 1
# Read CSV File
df = pd.read_csv("movie_dataset.csv")

#printing the dataset first 5
print (df.head())

#printing the columns
print (df.columns)

##Step 2
# Select Features
features = ['keywords', 'cast', 'genres', 'director']

##Step 3
# Create a column in DF which combines all selected features
for feature in features:
  df[feature] = df[feature].fillna('')

def combine_features(row):
  return row['keywords'] + " " + row['cast'] + row['genres'] + row['director']

df["combined_features"] = df.apply(combine_features, axis=1)

print ("Combined Features: \n", df["combined_features"].head())

##Setp 4
# Create a count matrix from this new combined column
cv = CountVectorizer()

count_matrix = cv.fit_transform(df["combined_features"])

##Step 5
# Compute the cosine similarity based on the count_matrix
cosine_sim = cosine_similarity(count_matrix)

movie_user_like = "Avatar"

##Step 6:
# Get index of the user movie from its title
movie_index = get_index_from_title(movie_user_like)

similar_movies = list(enumerate(cosine_sim[movie_index]))

##Step 7:
# Get a list of similar movies in descending order of similartiy score
sorted_similar_movies = sorted(similar_movies, key= lambda x:x[1],reverse=True)

##Step 8:
# Print titles of first 50 movies
i=0
for movie in sorted_similar_movies:
  print (get_title_from_index(movie[0]))
  i=i+i
  if i>50:
    break

